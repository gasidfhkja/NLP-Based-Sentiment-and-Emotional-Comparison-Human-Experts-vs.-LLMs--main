# -*- coding: utf-8 -*-
"""BTP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I56j0Nsq9zJrfJOkkZe-LZc9SpsCblKz

### **Loading and Previewing Data**
"""

import pandas as pd
df = pd.read_csv('/content/Response data All.csv')
df.head(10)

df.columns

"""### **Initial Data Exploration and Cleaning**

**Check for missing values**
"""

missing_values = df.isnull().sum()
print("Missing values in each column:\n", missing_values)
df.info()

"""### **Basic Descriptive Statistics and Preview**

**Raw Length of Responses**
"""

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

df['Human_Length_Raw'] = df['Human Answer'].apply(len)
df['ChatGPT_Length_Raw'] = df['OpenAI Chatgpt Answer'].apply(len)
df['Gemini_Length_Raw'] = df['Google Gemini Answer'].apply(len)
df['Copilot_Length_Raw'] = df['Microsoft Copilot Answer'].apply(len)
df['Claude_Length_Raw'] = df['Claude Answer'].apply(len)
df['Perplexity_Length_Raw'] = df['Perplexity Answer'].apply(len)

avg_raw_lengths = df[['Human_Length_Raw', 'ChatGPT_Length_Raw', 'Gemini_Length_Raw',
                      'Copilot_Length_Raw', 'Claude_Length_Raw', 'Perplexity_Length_Raw']].mean()

custom_labels = {
    'Human_Length_Raw': 'Human',
    'ChatGPT_Length_Raw': 'Chatgpt',
    'Gemini_Length_Raw': 'Gemini',
    'Copilot_Length_Raw': 'Copilot',
    'Claude_Length_Raw': 'Claude',
    'Perplexity_Length_Raw': 'Perplexity'
}

avg_raw_lengths.index = avg_raw_lengths.index.map(custom_labels)

lengths_raw_text = avg_raw_lengths.to_string()
print("Average Raw Length of Responses by Source (Text Output):\n", lengths_raw_text)

with open("average_raw_lengths_output.txt", "w") as file:
    file.write("Average Raw Length of Responses by Source:\n" + lengths_raw_text)

colors = ['#000000', '#74aa9c', '#6e7fd9', '#51bded', '#da7251', '#257d89']
plt.figure(figsize=(12, 6))
avg_raw_lengths.plot(kind='bar', color=colors, edgecolor='black', linewidth=1)
plt.title('Average Raw Length of Responses by Source', fontsize=18, fontweight='bold')
plt.ylabel('Average Response Length (characters)', fontsize=14)
plt.xlabel('Source', fontsize=14)

plt.xticks(rotation=0, fontsize=12,fontweight='bold')
plt.yticks(fontsize=12)

plt.grid(axis='y', linestyle='--', alpha=0.5)
sns.despine()

plt.tight_layout()
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Preparing the data in a long-form format
lengths_melted = df.melt(value_vars=['Human_Length_Raw', 'ChatGPT_Length_Raw',
                                     'Gemini_Length_Raw', 'Copilot_Length_Raw',
                                     'Claude_Length_Raw', 'Perplexity_Length_Raw'],
                         var_name='Source', value_name='Response Length')

# Mapping custom x-axis labels
custom_labels = {
    'Human_Length_Raw': 'Human',
    'ChatGPT_Length_Raw': 'Chatgpt',
    'Gemini_Length_Raw': 'Gemini',
    'Copilot_Length_Raw': 'Copilot',
    'Claude_Length_Raw': 'Claude',
    'Perplexity_Length_Raw': 'Perplexity'
}

# Replace the values in the 'Source' column with custom labels
lengths_melted['Source'] = lengths_melted['Source'].map(custom_labels)

# Updated color palette matching custom labels
color_palette = {
    'Human': 'black',
    'Chatgpt': '#74aa9c',
    'Gemini': '#6e7fd9',
    'Copilot': '#51bded',
    'Claude': '#da7251',
    'Perplexity': '#257d89'
}

# Setting a larger plot size for better visibility
plt.figure(figsize=(20, 8))

# Creating the violin plot with custom colors
sns.violinplot(x='Source', y='Response Length', data=lengths_melted,
               palette=color_palette, inner='quartile', linewidth=1.2)

# Adding mean points with `errorbar=None`
sns.pointplot(x='Source', y='Response Length', data=lengths_melted,
              estimator='mean', color='gray', markers='o', linestyles='-', errorbar=None)

# Adding titles and labels
plt.title('Violin Plot of Response Lengths by Source', fontsize=16, fontweight='bold')
plt.xlabel('Response', fontsize=14)
plt.ylabel('Response Length (characters)', fontsize=14)
plt.xticks(rotation=0, fontsize=12, fontweight='bold')
plt.yticks(fontsize=12)

# Adding a grid for better readability
plt.grid(axis='y', linestyle='--', alpha=0.5)

# Display the plot
plt.tight_layout()  # Ensures everything fits within the plot area
plt.show()

"""## **Preprocessing**

### **Installing required libraries**
NLTK (Natural Language Toolkit) for text processing, spaCy for tokenization and lemmatization, and download the spaCy language model.
"""

!pip install nltk spacy
!python -m spacy download en_core_web_sm

import re
import nltk
import spacy
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

# Load spaCy's English language model
nlp = spacy.load("en_core_web_sm")
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

"""### **Text Cleaning**
Cleaning the text by removing URLs, non-alphanumeric characters, and unnecessary white spaces.
"""

def clean_text(text):
    # Remove URLs
    text = re.sub(r'http\S+|www\S+|https\S+', '', text)
    # Remove non-alphanumeric characters (except spaces)
    text = re.sub(r'[^A-Za-z0-9\s]', '', text)
    # Remove multiple spaces
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# Example cleaning text
example_text = "Visit https://example.com for more details! It's really great."
cleaned_text = clean_text(example_text)
cleaned_text

"""### **Tokenization**
Breaking the text into words using spaCy and NLTK. We'll use NLTK for tokenization since it's simple for basic tasks.
"""

nltk.download('punkt_tab')

def tokenize_text(text):
    # Tokenize the text using NLTK
    tokens = word_tokenize(text)
    return tokens

# Example of tokenizing cleaned text
tokens = tokenize_text(cleaned_text)
tokens

"""### **Normalization**
Converting all tokens to lowercase for consistency.
"""

def normalize_text(tokens):
    # Convert tokens to lowercase
    tokens = [token.lower() for token in tokens]
    return tokens

normalized_tokens = normalize_text(tokens)
normalized_tokens

"""### **Stopword Removal**
Removing common words (stopwords) that do not add value to our analysis.


"""

def remove_stopwords(tokens):
    # Load NLTK's stopwords
    stop_words = set(stopwords.words('english'))
    # Remove stopwords
    tokens = [token for token in tokens if token not in stop_words]
    return tokens

filtered_tokens = remove_stopwords(normalized_tokens)
filtered_tokens

"""### **Lemmatization**
Reducing the words to their base form using spaCy’s lemmatizer.
"""

def lemmatize_tokens(tokens):
    # Lemmatize tokens using spaCy
    doc = nlp(" ".join(tokens))
    lemmatized_tokens = [token.lemma_ for token in doc]
    return lemmatized_tokens

lemmatized_tokens = lemmatize_tokens(filtered_tokens)
lemmatized_tokens

"""### **Applying Preprocessing to Text Columns**"""

import pandas as pd

# Load the CSV data (Replace 'your_file.csv' with your actual file path or URL)
df = pd.read_csv('/content/Response data All.csv')

# Clean text function
df['Clean_Human_Answer'] = df['Human Answer'].apply(clean_text)
df['Clean_ChatGPT_Answer'] = df['OpenAI Chatgpt Answer'].apply(clean_text)
df['Clean_Gemini_Answer'] = df['Google Gemini Answer'].apply(clean_text)
df['Clean_Copilot_Answer'] = df['Microsoft Copilot Answer'].apply(clean_text)
df['Clean_Claude_Answer'] = df['Claude Answer'].apply(clean_text)
df['Clean_Perplexity_Answer'] = df['Perplexity Answer'].apply(clean_text)

# Tokenize text
df['Tokenized_Human_Answer'] = df['Clean_Human_Answer'].apply(tokenize_text)
df['Tokenized_ChatGPT_Answer'] = df['Clean_ChatGPT_Answer'].apply(tokenize_text)
df['Tokenized_Gemini_Answer'] = df['Clean_Gemini_Answer'].apply(tokenize_text)
df['Tokenized_Copilot_Answer'] = df['Clean_Copilot_Answer'].apply(tokenize_text)
df['Tokenized_Claude_Answer'] = df['Clean_Claude_Answer'].apply(tokenize_text)
df['Tokenized_Perplexity_Answer'] = df['Clean_Perplexity_Answer'].apply(tokenize_text)

# Normalize (Lowercase)
df['Normalized_Human_Answer'] = df['Tokenized_Human_Answer'].apply(normalize_text)
df['Normalized_ChatGPT_Answer'] = df['Tokenized_ChatGPT_Answer'].apply(normalize_text)
df['Normalized_Gemini_Answer'] = df['Tokenized_Gemini_Answer'].apply(normalize_text)
df['Normalized_Copilot_Answer'] = df['Tokenized_Copilot_Answer'].apply(normalize_text)
df['Normalized_Claude_Answer'] = df['Tokenized_Claude_Answer'].apply(normalize_text)
df['Normalized_Perplexity_Answer'] = df['Tokenized_Perplexity_Answer'].apply(normalize_text)

# Remove stopwords
df['Filtered_Human_Answer'] = df['Normalized_Human_Answer'].apply(remove_stopwords)
df['Filtered_ChatGPT_Answer'] = df['Normalized_ChatGPT_Answer'].apply(remove_stopwords)
df['Filtered_Gemini_Answer'] = df['Normalized_Gemini_Answer'].apply(remove_stopwords)
df['Filtered_Copilot_Answer'] = df['Normalized_Copilot_Answer'].apply(remove_stopwords)
df['Filtered_Claude_Answer'] = df['Normalized_Claude_Answer'].apply(remove_stopwords)
df['Filtered_Perplexity_Answer'] = df['Normalized_Perplexity_Answer'].apply(remove_stopwords)

# Lemmatize
df['Lemmatized_Human_Answer'] = df['Filtered_Human_Answer'].apply(lemmatize_tokens)
df['Lemmatized_ChatGPT_Answer'] = df['Filtered_ChatGPT_Answer'].apply(lemmatize_tokens)
df['Lemmatized_Gemini_Answer'] = df['Filtered_Gemini_Answer'].apply(lemmatize_tokens)
df['Lemmatized_Copilot_Answer'] = df['Filtered_Copilot_Answer'].apply(lemmatize_tokens)
df['Lemmatized_Claude_Answer'] = df['Filtered_Claude_Answer'].apply(lemmatize_tokens)
df['Lemmatized_Perplexity_Answer'] = df['Filtered_Perplexity_Answer'].apply(lemmatize_tokens)

# Show the updated dataframe with the preprocessed text columns
df.head()

"""### **Length of Preprocessed Text(After Preprocessing)**"""

# Calculate the length of the preprocessed responses
df['Human_Length_Cleaned'] = df['Lemmatized_Human_Answer'].apply(len)
df['ChatGPT_Length_Cleaned'] = df['Lemmatized_ChatGPT_Answer'].apply(len)
df['Gemini_Length_Cleaned'] = df['Lemmatized_Gemini_Answer'].apply(len)
df['Copilot_Length_Cleaned'] = df['Lemmatized_Copilot_Answer'].apply(len)
df['Claude_Length_Cleaned'] = df['Lemmatized_Claude_Answer'].apply(len)
df['Perplexity_Length_Cleaned'] = df['Lemmatized_Perplexity_Answer'].apply(len)

# Create a DataFrame to hold average lengths of cleaned responses
avg_cleaned_lengths = df[['Human_Length_Cleaned', 'ChatGPT_Length_Cleaned', 'Gemini_Length_Cleaned',
                          'Copilot_Length_Cleaned', 'Claude_Length_Cleaned', 'Perplexity_Length_Cleaned']].mean()

# Mapping custom x-axis labels as before
custom_labels = {
    'Human_Length_Cleaned': 'Human',
    'ChatGPT_Length_Cleaned': 'Chatgpt',
    'Gemini_Length_Cleaned': 'Gemini',
    'Copilot_Length_Cleaned': 'Copilot',
    'Claude_Length_Cleaned': 'Claude',
    'Perplexity_Length_Cleaned': 'Perplexity'
}

# Renaming indices in the average length series for custom labeling
avg_cleaned_lengths.index = avg_cleaned_lengths.index.map(custom_labels)

# Convert average lengths to a text-friendly output
lengths_cleaned_text = avg_cleaned_lengths.to_string()
print("Avg Cleaned lengths of Responses by Source (Text Output):\n", lengths_cleaned_text)

# Save the text output to a file for easy sharing
with open("avg_cleaned_lengths_output.txt", "w") as file:
    file.write("Average Cleaned Length of Responses by Source:\n" + lengths_cleaned_text)

# Define colors for each source with black for Human
colors = ['#000000', '#74aa9c', '#6e7fd9', '#51bded', '#da7251', '#257d89']

# Set plot style and create a bar chart for average cleaned lengths
plt.figure(figsize=(12, 6))
avg_cleaned_lengths.plot(kind='bar', color=colors, edgecolor='black', linewidth=1)

# Enhance plot title and labels
plt.title('Average Length of Preprocessed Responses by Source' ,fontsize=18, fontweight='bold')
plt.ylabel('Average Response Length (characters)', fontsize=14)
plt.xlabel('Source', fontsize=14)

# Customize ticks for better readability
plt.xticks(rotation=0, fontsize=12,fontweight='bold')
plt.yticks(fontsize=12)

# Add grid lines and style adjustments
plt.grid(axis='y', linestyle='--', alpha=0.5)
sns.despine()

# Show the plot with tight layout for clarity
plt.tight_layout()
plt.show()

"""## **Natural Language Understanding (NLU) and Analysis**


---



Sentiment analysis is a technique in Natural Language Processing (NLP) that detects the emotional tone or attitude in a piece of text. It typically classifies text as positive, negative, or neutral, often with a numeric score to reflect the intensity of these sentiments.

### **Sentiment Analysis**


---

 The aim is to compare the sentiment of AI-generated answers with human responses, ideally to see how closely these AI answers match the sentiment expressed by real human experts.

**Install and Import of Required Libraries**
"""

# Install the necessary libraries
!pip install vaderSentiment
!pip install textblob

# Importing libraries
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from textblob import TextBlob
import pandas as pd
import matplotlib.pyplot as plt

"""**Sentiment Analysis with VADER**


---

>Valence Aware Dictionary and Sentiment Reasoner
"""

# Initialize VADER sentiment intensity analyzer
analyzer = SentimentIntensityAnalyzer()

# Function to calculate sentiment using VADER
def get_vader_sentiment(text):
    sentiment = analyzer.polarity_scores(text)
    return sentiment['compound']  # This returns the compound score

# Apply the function to each relevant column in the DataFrame
df['Human_Sentiment_Vader'] = df['Clean_Human_Answer'].apply(get_vader_sentiment)
df['ChatGPT_Sentiment_Vader'] = df['Clean_ChatGPT_Answer'].apply(get_vader_sentiment)
df['Gemini_Sentiment_Vader'] = df['Clean_Gemini_Answer'].apply(get_vader_sentiment)
df['Copilot_Sentiment_Vader'] = df['Clean_Copilot_Answer'].apply(get_vader_sentiment)
df['Claude_Sentiment_Vader'] = df['Clean_Claude_Answer'].apply(get_vader_sentiment)
df['Perplexity_Sentiment_Vader'] = df['Clean_Perplexity_Answer'].apply(get_vader_sentiment)

# Display the sentiment results
df[['Human_Sentiment_Vader', 'ChatGPT_Sentiment_Vader', 'Gemini_Sentiment_Vader',
    'Copilot_Sentiment_Vader', 'Claude_Sentiment_Vader', 'Perplexity_Sentiment_Vader']].head()

"""**Sentiment Analysis with TextBlob**


---


>**VADER** is generally more tailored for **social media-like** text, **TextBlob** uses a simpler approach for **polarity and subjectivity analysis**.
"""

# Function to calculate sentiment using TextBlob
def get_textblob_sentiment(text):
    blob = TextBlob(text)
    return blob.sentiment.polarity  # Polarity ranges from -1 to 1

# Apply TextBlob sentiment to each relevant column
df['Human_Sentiment_TextBlob'] = df['Clean_Human_Answer'].apply(get_textblob_sentiment)
df['ChatGPT_Sentiment_TextBlob'] = df['Clean_ChatGPT_Answer'].apply(get_textblob_sentiment)
df['Gemini_Sentiment_TextBlob'] = df['Clean_Gemini_Answer'].apply(get_textblob_sentiment)
df['Copilot_Sentiment_TextBlob'] = df['Clean_Copilot_Answer'].apply(get_textblob_sentiment)
df['Claude_Sentiment_TextBlob'] = df['Clean_Claude_Answer'].apply(get_textblob_sentiment)
df['Perplexity_Sentiment_TextBlob'] = df['Clean_Perplexity_Answer'].apply(get_textblob_sentiment)

# Display the TextBlob sentiment results
df[['Human_Sentiment_TextBlob', 'ChatGPT_Sentiment_TextBlob', 'Gemini_Sentiment_TextBlob',
    'Copilot_Sentiment_TextBlob', 'Claude_Sentiment_TextBlob', 'Perplexity_Sentiment_TextBlob']].head()

"""**Sentiment Visualization**


---


"""

import matplotlib.pyplot as plt

# Colors for each source (Human in black and others with the given color palette)
colors = ['black', '#74aa9c', '#6e7fd9', '#51bded', '#da7251', '#257d89']
labels = ['Human', 'ChatGPT', 'Gemini', 'Copilot', 'Claude', 'Perplexity']  # Custom labels for x-axis

# Visualize VADER sentiment scores
vader_sentiments = ['Human_Sentiment_Vader', 'ChatGPT_Sentiment_Vader', 'Gemini_Sentiment_Vader',
                    'Copilot_Sentiment_Vader', 'Claude_Sentiment_Vader', 'Perplexity_Sentiment_Vader']

# Plotting VADER sentiment scores
plt.figure(figsize=(12, 6))
plt.bar(labels, df[vader_sentiments].mean(), color=colors)
plt.title('Average VADER Sentiment Scores', fontsize=16, fontweight='bold')
plt.ylabel('Sentiment Score (Compound)', fontsize=14)
plt.xlabel('Response Source', fontsize=14)
plt.xticks(rotation=0, fontsize=12, fontweight='bold')
plt.yticks(fontsize=12)
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# Visualize TextBlob sentiment scores
textblob_sentiments = ['Human_Sentiment_TextBlob', 'ChatGPT_Sentiment_TextBlob', 'Gemini_Sentiment_TextBlob',
                       'Copilot_Sentiment_TextBlob', 'Claude_Sentiment_TextBlob', 'Perplexity_Sentiment_TextBlob']

# Plotting TextBlob sentiment scores
plt.figure(figsize=(12, 6))
plt.bar(labels, df[textblob_sentiments].mean(), color=colors)
plt.title('Average TextBlob Sentiment Scores', fontsize=16, fontweight='bold')
plt.ylabel('Sentiment Score (Polarity)', fontsize=14)
plt.xlabel('Response Source', fontsize=14, fontweight='bold')
plt.xticks(rotation=0, fontsize=12)
plt.yticks(fontsize=12)
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# Save sentiment results
sentiment_columns = ['Serial No.', 'Human_Sentiment_Vader', 'ChatGPT_Sentiment_Vader', 'Gemini_Sentiment_Vader',
                     'Copilot_Sentiment_Vader', 'Claude_Sentiment_Vader', 'Perplexity_Sentiment_Vader',
                     'Human_Sentiment_TextBlob', 'ChatGPT_Sentiment_TextBlob', 'Gemini_Sentiment_TextBlob',
                     'Copilot_Sentiment_TextBlob', 'Claude_Sentiment_TextBlob', 'Perplexity_Sentiment_TextBlob']

df[sentiment_columns].to_csv('sentiment_analysis_results.csv', index=False)

"""**Average sentiments**

---


"""

# Calculate and print the average sentiment scores for each source using VADER
vader_sentiments = {
    'Human': df['Human_Sentiment_Vader'].mean(),
    'ChatGPT': df['ChatGPT_Sentiment_Vader'].mean(),
    'Gemini': df['Gemini_Sentiment_Vader'].mean(),
    'Copilot': df['Copilot_Sentiment_Vader'].mean(),
    'Claude': df['Claude_Sentiment_Vader'].mean(),
    'Perplexity': df['Perplexity_Sentiment_Vader'].mean()
}
vader_sentiment_text = '\n'.join([f"{source}_Sentiment_VADER: {score:.4f}" for source, score in vader_sentiments.items()])
print("Average VADER Sentiment Scores by Source (Text Output):\n", vader_sentiment_text)

# Calculate and print the average sentiment scores for each source using TextBlob
textblob_sentiments = {
    'Human': df['Human_Sentiment_TextBlob'].mean(),
    'ChatGPT': df['ChatGPT_Sentiment_TextBlob'].mean(),
    'Gemini': df['Gemini_Sentiment_TextBlob'].mean(),
    'Copilot': df['Copilot_Sentiment_TextBlob'].mean(),
    'Claude': df['Claude_Sentiment_TextBlob'].mean(),
    'Perplexity': df['Perplexity_Sentiment_TextBlob'].mean()
}
textblob_sentiment_text = '\n'.join([f"{source}_Sentiment_TextBlob: {score:.4f}" for source, score in textblob_sentiments.items()])
print("Average TextBlob Sentiment Scores by Source (Text Output):\n", textblob_sentiment_text)

# Optional: Save to text files for easier sharing or referencing
with open("vader_sentiment_scores.txt", "w") as file:
    file.write("Average VADER Sentiment Scores by Source:\n" + vader_sentiment_text)

with open("textblob_sentiment_scores.txt", "w") as file:
    file.write("Average TextBlob Sentiment Scores by Source:\n" + textblob_sentiment_text)

"""**Sentiment deviation from human responses**"""

# Calculate combined sentiment score as the average of VADER and TextBlob scores for each response source
df['Human_Combined_Sentiment'] = (df['Human_Sentiment_Vader'] + df['Human_Sentiment_TextBlob']) / 2
df['ChatGPT_Combined_Sentiment'] = (df['ChatGPT_Sentiment_Vader'] + df['ChatGPT_Sentiment_TextBlob']) / 2
df['Gemini_Combined_Sentiment'] = (df['Gemini_Sentiment_Vader'] + df['Gemini_Sentiment_TextBlob']) / 2
df['Copilot_Combined_Sentiment'] = (df['Copilot_Sentiment_Vader'] + df['Copilot_Sentiment_TextBlob']) / 2
df['Claude_Combined_Sentiment'] = (df['Claude_Sentiment_Vader'] + df['Claude_Sentiment_TextBlob']) / 2
df['Perplexity_Combined_Sentiment'] = (df['Perplexity_Sentiment_Vader'] + df['Perplexity_Sentiment_TextBlob']) / 2

# Calculate absolute deviation from human sentiment for each AI response
df['ChatGPT_Deviation'] = abs(df['Human_Combined_Sentiment'] - df['ChatGPT_Combined_Sentiment'])
df['Gemini_Deviation'] = abs(df['Human_Combined_Sentiment'] - df['Gemini_Combined_Sentiment'])
df['Copilot_Deviation'] = abs(df['Human_Combined_Sentiment'] - df['Copilot_Combined_Sentiment'])
df['Claude_Deviation'] = abs(df['Human_Combined_Sentiment'] - df['Claude_Combined_Sentiment'])
df['Perplexity_Deviation'] = abs(df['Human_Combined_Sentiment'] - df['Perplexity_Combined_Sentiment'])

# Calculate average deviation and combined sentiment for final text output
average_combined_sentiment = df[['Human_Combined_Sentiment', 'ChatGPT_Combined_Sentiment',
                                 'Gemini_Combined_Sentiment', 'Copilot_Combined_Sentiment',
                                 'Claude_Combined_Sentiment', 'Perplexity_Combined_Sentiment']].mean()
average_deviation = df[['ChatGPT_Deviation', 'Gemini_Deviation', 'Copilot_Deviation',
                        'Claude_Deviation', 'Perplexity_Deviation']].mean()

# Print text output for easy review and sharing
print("Average Combined Sentiment Scores by Source (Text Output):\n", average_combined_sentiment.to_string())
print("\nAverage Sentiment Deviation from Human Responses (Text Output):\n", average_deviation.to_string())

"""---
***While VADER and TextBlob are useful, they are not always tailored to handle formal or corporate language. Fine-tuning more advanced models like FinBERT or using transformers allows the model to learn from the specific language used in corporate contexts.***

**Sentiment Analysis with FinBERT**


---

>**Business-Specific Sentiment:** FinBERT is pre-trained on financial news and reports, making it particularly suited for analyzing corporate, financial, and business-related text.

>**Accuracy:** It can understand the subtleties of corporate language better than general sentiment analysis models like VADER and TextBlob.
"""

!pip install transformers torch

import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import pandas as pd
import numpy as np

# Load FinBERT tokenizer and model
tokenizer = AutoTokenizer.from_pretrained("yiyanghkust/finbert-tone")
model = AutoModelForSequenceClassification.from_pretrained("yiyanghkust/finbert-tone")

"""**Why Use Pre-Trained Models & Auto-Tokenizer**
* **Pre-trained models** like **FinBERT** are specifically tuned for sentiment analysis, especially in domains like finance and corporate language, making them ideal for analyzing the corporate-style language in our data.

* **Auto-tokenizers** ensure that text is transformed into a format compatible with the model. They automatically handle necessary tokenization, special tokens, padding, and truncation, reducing preprocessing errors and ensuring compatibility with the pre-trained model’s vocabulary.


"""

# Define the function to apply sentiment analysis using FinBERT
def finbert_sentiment_analysis(text):
    # Tokenize the input text with padding, truncation, and max_length
    inputs = tokenizer(text,
                       padding=True,
                       truncation=True,
                       max_length=512,  # Max length is set to 512
                       return_tensors="pt")  # Return as PyTorch tensors

    # Perform inference with the FinBERT model
    with torch.no_grad():
        outputs = model(**inputs)

    # Get the sentiment prediction (0: negative, 1: neutral, 2: positive)
    sentiment = torch.argmax(outputs.logits, dim=1).item()

    # Return sentiment score (positive, neutral, or negative sentiment)
    return sentiment

# Apply FinBERT sentiment analysis on each cleaned answer column
df['Human_Sentiment_FinBERT'] = df['Clean_Human_Answer'].apply(finbert_sentiment_analysis)
df['ChatGPT_Sentiment_FinBERT'] = df['Clean_ChatGPT_Answer'].apply(finbert_sentiment_analysis)
df['Gemini_Sentiment_FinBERT'] = df['Clean_Gemini_Answer'].apply(finbert_sentiment_analysis)
df['Copilot_Sentiment_FinBERT'] = df['Clean_Copilot_Answer'].apply(finbert_sentiment_analysis)
df['Claude_Sentiment_FinBERT'] = df['Clean_Claude_Answer'].apply(finbert_sentiment_analysis)
df['Perplexity_Sentiment_FinBERT'] = df['Clean_Perplexity_Answer'].apply(finbert_sentiment_analysis)

# Display the average sentiment scores for FinBERT-based sentiment analysis
average_finbert_sentiments = df[['Human_Sentiment_FinBERT', 'ChatGPT_Sentiment_FinBERT', 'Gemini_Sentiment_FinBERT',
                                 'Copilot_Sentiment_FinBERT', 'Claude_Sentiment_FinBERT', 'Perplexity_Sentiment_FinBERT']].mean()

print("Average FinBERT Sentiment Scores by Source (Text Output):\n", average_finbert_sentiments.to_string())

import matplotlib.pyplot as plt

# Create a list of sentiment scores and labels
sentiment_scores = [
    average_finbert_sentiments['Human_Sentiment_FinBERT'],
    average_finbert_sentiments['ChatGPT_Sentiment_FinBERT'],
    average_finbert_sentiments['Gemini_Sentiment_FinBERT'],
    average_finbert_sentiments['Copilot_Sentiment_FinBERT'],
    average_finbert_sentiments['Claude_Sentiment_FinBERT'],
    average_finbert_sentiments['Perplexity_Sentiment_FinBERT']
]

labels = ['Human', 'ChatGPT', 'Gemini', 'Copilot', 'Claude', 'Perplexity']

# Create a bar chart
plt.figure(figsize=(10, 6))
plt.bar(labels, sentiment_scores, color=['black', '#74aa9c', '#6e7fd9', '#51bded', '#da7251', '#257d89'])

# Adding title and labels
plt.title('Average FinBERT Sentiment Scores by Source', fontsize=16, fontweight='bold' )
plt.xlabel('Response Source', fontsize=12, fontweight='bold')
plt.ylabel('Sentiment Score', fontsize=12, fontweight='bold')

# Show the plot
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Assuming df contains FinBERT sentiment scores for each answer instance
# Step 1: Extract relevant sentiment columns
finbert_sentiments = df[['Human_Sentiment_FinBERT', 'ChatGPT_Sentiment_FinBERT',
                         'Gemini_Sentiment_FinBERT', 'Copilot_Sentiment_FinBERT',
                         'Claude_Sentiment_FinBERT', 'Perplexity_Sentiment_FinBERT']]

# Step 2: Define a function to label sentiment based on the score
def label_sentiment(score):
    if score > 0.15:
        return "Very Positive"
    elif score > 0.05:
        return "Positive"
    elif score >= -0.05 and score <= 0.05:
        return "Neutral"
    elif score < -0.15:
        return "Very Negative"
    else:
        return "Negative"

# Step 3: Apply the labeling function to each score
labeled_sentiments = finbert_sentiments.applymap(label_sentiment)

# Step 4: Count the occurrences of each sentiment label for each source
sentiment_counts = labeled_sentiments.apply(pd.Series.value_counts).fillna(0).astype(int).T

# Step 5: Plot the heatmap with counts of each sentiment label by source
plt.figure(figsize=(10, 6))
sns.heatmap(sentiment_counts, annot=True, cmap="Blues", fmt='d', cbar=False)

# Customizing the x and y labels
plt.title('Sentiment Label Counts by Source', fontsize=16, fontweight='bold')
plt.xlabel('Sentiment Label', fontsize=14)
plt.ylabel('Response Source', fontsize=14)
plt.xticks(rotation=0)  # Rotate sentiment labels for readability
plt.yticks(rotation=0)   # Keep sources vertical for readability

plt.tight_layout()
plt.show()

"""**SAVING RESULTS**"""

# Save sentiment results
sentiment_columns = ['Serial No.', 'Human_Sentiment_Vader', 'ChatGPT_Sentiment_Vader', 'Gemini_Sentiment_Vader',
                     'Copilot_Sentiment_Vader', 'Claude_Sentiment_Vader', 'Perplexity_Sentiment_Vader',
                     'Human_Sentiment_TextBlob', 'ChatGPT_Sentiment_TextBlob', 'Gemini_Sentiment_TextBlob',
                     'Copilot_Sentiment_TextBlob', 'Claude_Sentiment_TextBlob', 'Perplexity_Sentiment_TextBlob',
                     'Human_Sentiment_FinBERT', 'ChatGPT_Sentiment_FinBERT', 'Gemini_Sentiment_FinBERT',
                     'Copilot_Sentiment_FinBERT', 'Claude_Sentiment_FinBERT', 'Perplexity_Sentiment_FinBERT']

df[sentiment_columns].to_csv('sentiment_analysis_results.csv', index=False)

import pandas as pd

# Load sentiment data from CSV file
df = pd.read_csv('sentiment_analysis_results.csv')

# Calculate average sentiment scores for each source (across all questions)
average_sentiments = df[['Human_Sentiment_Vader', 'ChatGPT_Sentiment_Vader', 'Gemini_Sentiment_Vader',
                         'Copilot_Sentiment_Vader', 'Claude_Sentiment_Vader', 'Perplexity_Sentiment_Vader',
                         'Human_Sentiment_TextBlob', 'ChatGPT_Sentiment_TextBlob', 'Gemini_Sentiment_TextBlob',
                         'Copilot_Sentiment_TextBlob', 'Claude_Sentiment_TextBlob', 'Perplexity_Sentiment_TextBlob',
                         'Human_Sentiment_FinBERT', 'ChatGPT_Sentiment_FinBERT', 'Gemini_Sentiment_FinBERT',
                         'Copilot_Sentiment_FinBERT', 'Claude_Sentiment_FinBERT', 'Perplexity_Sentiment_FinBERT']].mean()

# Reshape the average sentiment scores into the desired format (speaker_sentiment_model, score)
average_sentiments_df = average_sentiments.reset_index()
average_sentiments_df.columns = ['speaker_sentiment_model', 'score']

# Save the restructured average sentiment scores to a CSV file
average_sentiments_df.to_csv('average_sentiment_scores.csv', index=False)

"""**Final Analysis**

---


"""

# 2. Line Plot Comparison (Human vs AI)
# First, filter out the relevant sentiment columns for plotting comparison
human_columns = ['Human_Sentiment_Vader', 'Human_Sentiment_TextBlob', 'Human_Sentiment_FinBERT']
ai_columns = ['ChatGPT_Sentiment_Vader', 'ChatGPT_Sentiment_TextBlob', 'ChatGPT_Sentiment_FinBERT',
              'Gemini_Sentiment_Vader', 'Gemini_Sentiment_TextBlob', 'Gemini_Sentiment_FinBERT',
              'Copilot_Sentiment_Vader', 'Copilot_Sentiment_TextBlob', 'Copilot_Sentiment_FinBERT',
              'Claude_Sentiment_Vader', 'Claude_Sentiment_TextBlob', 'Claude_Sentiment_FinBERT',
              'Perplexity_Sentiment_Vader', 'Perplexity_Sentiment_TextBlob', 'Perplexity_Sentiment_FinBERT']

# Plotting Human vs AI (across VADER, TextBlob, FinBERT)
plt.figure(figsize=(12, 6))
plt.plot(average_sentiments_df['speaker_sentiment_model'], average_sentiments_df['score'], label='All Models', marker='o', color='blue')
plt.plot(human_columns, average_sentiments[human_columns], label='Human Sentiment', color='green', linestyle='--', marker='o')
plt.plot(ai_columns, average_sentiments[ai_columns], label='AI Models', color='red', linestyle='--', marker='o')

plt.title('Human vs AI Sentiment Scores Comparison (Across Methods)', fontsize=14)
plt.xlabel('Sentiment Model', fontsize=12)
plt.ylabel('Average Sentiment Score', fontsize=12)
plt.xticks(rotation=45, ha='right')
plt.legend(loc='upper left')
plt.tight_layout()
plt.show()

""">We first extract the average sentiment scores for the AI models and Human for each of the three methods: VADER, TextBlob, and FinBERT.

>The AI average is calculated as the mean of the 5 AI model scores for each method.

>The Human average is calculated for each method's human response.
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the average sentiment scores from the CSV file
df = pd.read_csv('average_sentiment_scores.csv')

# Extracting and preparing the data
# AI Sentiment for each method (Average of 5 AI models)
ai_vader = df.loc[df['speaker_sentiment_model'].str.contains('Sentiment_Vader') & df['speaker_sentiment_model'].str.contains('ChatGPT|Gemini|Copilot|Claude|Perplexity'), 'score'].mean()
ai_textblob = df.loc[df['speaker_sentiment_model'].str.contains('Sentiment_TextBlob') & df['speaker_sentiment_model'].str.contains('ChatGPT|Gemini|Copilot|Claude|Perplexity'), 'score'].mean()
ai_finbert = df.loc[df['speaker_sentiment_model'].str.contains('Sentiment_FinBERT') & df['speaker_sentiment_model'].str.contains('ChatGPT|Gemini|Copilot|Claude|Perplexity'), 'score'].mean()

# Human Sentiment for each method
human_vader = df.loc[df['speaker_sentiment_model'] == 'Human_Sentiment_Vader', 'score'].mean()
human_textblob = df.loc[df['speaker_sentiment_model'] == 'Human_Sentiment_TextBlob', 'score'].mean()
human_finbert = df.loc[df['speaker_sentiment_model'] == 'Human_Sentiment_FinBERT', 'score'].mean()

# Prepare the data for visualization
sentiment_comparison = {
    'Method': ['VADER', 'TextBlob', 'FinBERT'],
    'AI Average': [ai_vader, ai_textblob, ai_finbert],
    'Human Average': [human_vader, human_textblob, human_finbert]
}

comparison_df = pd.DataFrame(sentiment_comparison)

# Plot the grouped bar chart for AI vs Human across all 3 methods
plt.figure(figsize=(12, 6))
comparison_df.set_index('Method').plot(kind='bar', width=0.6, color=['skyblue', 'lightcoral'], edgecolor='black')
plt.title('AI Average Sentiment vs Human Average Sentiment Across 3 Models', fontsize=12)
plt.ylabel('Average Sentiment Score', fontsize=12)
plt.xlabel('Sentiment Analysis Method', fontsize=12)
plt.xticks(rotation=0)
plt.tight_layout()
plt.legend(title='Sentiment', labels=['AI Average', 'Human Average'])
plt.show()

"""
**Final Analysis and Conclusion:**


---

>**Why FinBERT is Good for Analysis:**
- **VADER** and **TextBlob** are good for general sentiment analysis but may not capture the subtleties of corporate tone, which **FinBERT** is designed to handle better.
- **FinBERT** aligns more closely with the corporate language used by human respondents, while **VADER** and **TextBlob** may misinterpret the neutrality or formality of corporate responses, leading to skewed sentiment scores.
- **FinBERT’s** higher variation in sentiment between human and AI responses gives a clearer distinction for deeper analysis of **AI model biases**.

>**Key Observations:**
- **AI Models Tend to Be More Positive:** Across all three methods, AI models often show higher sentiment scores, especially with **VADER** and **FinBERT**, indicating they generate more optimistic responses.
- **TextBlob is Less Effective for Corporate Sentiment:** TextBlob’s lower sentiment scores for all sources suggest it's not well-suited for analyzing corporate or formal language.
- **FinBERT is Ideal for Corporate Language:** It is best suited for this task, offering insights into AI vs. human sentiment, and identifying where AI might exaggerate or misinterpret tone.
"""

print(df.columns)

import pandas as pd
import matplotlib.pyplot as plt

# Load the saved sentiment data
df2 = pd.read_csv('/content/sentiment_analysis_results.csv')

# Plot configurations
plt.figure(figsize=(15, 20))

# Define the models and their respective columns in the dataset
models = {
    "VADER": ['Human_Sentiment_Vader', 'ChatGPT_Sentiment_Vader', 'Gemini_Sentiment_Vader',
              'Copilot_Sentiment_Vader', 'Claude_Sentiment_Vader', 'Perplexity_Sentiment_Vader'],
    "TextBlob": ['Human_Sentiment_TextBlob', 'ChatGPT_Sentiment_TextBlob', 'Gemini_Sentiment_TextBlob',
                 'Copilot_Sentiment_TextBlob', 'Claude_Sentiment_TextBlob', 'Perplexity_Sentiment_TextBlob'],
    "FinBERT": ['Human_Sentiment_FinBERT', 'ChatGPT_Sentiment_FinBERT', 'Gemini_Sentiment_FinBERT',
                'Copilot_Sentiment_FinBERT', 'Claude_Sentiment_FinBERT', 'Perplexity_Sentiment_FinBERT']
}

# Colors for each AI model and human response
colors = {'Human': 'black', 'ChatGPT': 'blue', 'Gemini': 'green', 'Copilot': 'purple',
          'Claude': 'orange', 'Perplexity': 'red'}

# Iterate over each model and create a separate line plot
for i, (model_name, columns) in enumerate(models.items()):
    plt.subplot(3, 1, i+1)

    # Plot each AI and human sentiment score for each question
    for col in columns:
        if 'Human' in col:
            plt.plot(df2['Serial No.'], df2[col], label='Human', color=colors['Human'], linewidth=2)
        else:
            plt.plot(df2['Serial No.'], df2[col], label=col.split('_')[0], color=colors[col.split('_')[0]], linestyle='--')

    # Set the plot's title and labels
    plt.title(f'{model_name} Sentiment Variation per Question')
    plt.xlabel('Question Number')
    plt.ylabel(f'{model_name} Sentiment Score')
    plt.legend(loc='upper left', bbox_to_anchor=(1, 1), title='Source')
    plt.tight_layout()

# Display the plots
plt.show()

"""### **Emotion Analysis**

***Before diving into Emotional Analysis, it's crucial to understand why we proceed in this order.***

>**Sentiment Analysis** gives us a high-level view of whether a piece of text expresses a positive, negative, or neutral sentiment. This can help gauge the overall tone of responses. However, sentiment analysis is limited because it does not delve deeply into the specific emotions (like joy, fear, sadness, etc.) behind the text.

>**Emotional Analysis**, on the other hand, goes a step further. It helps identify complex emotional states beyond just sentiment polarity. For example, while sentiment analysis might classify a sentence as "positive," emotional analysis will break it down into emotions like joy, excitement, surprise, trust, etc., which gives a more nuanced understanding of the emotional tone of the response.
"""

!pip install text2emotion

!pip install emoji==1.7.0

import pandas as pd

# Load data from the main file
df = pd.read_csv('/content/preprocessed_responses.csv')
print(df.columns)

# If not installed, install necessary libraries
!pip install text2emotion transformers torch nltk

from transformers import pipeline, AutoTokenizer
import pandas as pd

# Load the tokenizer and emotion model pipeline
tokenizer = AutoTokenizer.from_pretrained("j-hartmann/emotion-english-distilroberta-base")
emotion_model = pipeline("text-classification", model="j-hartmann/emotion-english-distilroberta-base", tokenizer=tokenizer)

def batch_emotion_analysis(texts, max_tokens=512):
    """
    Analyze emotions for texts in batches, ensuring each text is truncated to fit the model's token limit.
    """
    results = []
    current_batch = []
    current_token_count = 0

    for text in texts:
        # Truncate text to fit within max token limit
        encoded_text = tokenizer(text, truncation=True, max_length=max_tokens, return_tensors="pt")
        truncated_text = tokenizer.decode(encoded_text['input_ids'][0], skip_special_tokens=True)

        # Calculate the token count for the truncated text
        token_count = len(encoded_text['input_ids'][0])

        # Check if adding this truncated text would exceed the max token limit for the batch
        if current_token_count + token_count > max_tokens:
            # Process the current batch if adding the text exceeds the limit
            batch_results = emotion_model(current_batch, truncation=True, max_length=max_tokens)
            results.extend(batch_results)

            # Reset the batch and token count for the next set
            current_batch = [truncated_text]
            current_token_count = token_count
        else:
            # Add truncated text to the current batch and update the token count
            current_batch.append(truncated_text)
            current_token_count += token_count

    # Process any remaining texts in the last batch
    if current_batch:
        batch_results = emotion_model(current_batch, truncation=True, max_length=max_tokens)
        results.extend(batch_results)

    return results

# Apply batch processing to each model's cleaned answers
df['Human_Emotion_Specialized'] = batch_emotion_analysis(df['Clean_Human_Answer'].tolist())
df['ChatGPT_Emotion_Specialized'] = batch_emotion_analysis(df['Clean_ChatGPT_Answer'].tolist())
df['Gemini_Emotion_Specialized'] = batch_emotion_analysis(df['Clean_Gemini_Answer'].tolist())
df['Copilot_Emotion_Specialized'] = batch_emotion_analysis(df['Clean_Copilot_Answer'].tolist())
df['Claude_Emotion_Specialized'] = batch_emotion_analysis(df['Clean_Claude_Answer'].tolist())
df['Perplexity_Emotion_Specialized'] = batch_emotion_analysis(df['Clean_Perplexity_Answer'].tolist())

df[['Human_Emotion_Specialized','ChatGPT_Emotion_Specialized','Gemini_Emotion_Specialized',
  'Copilot_Emotion_Specialized','Claude_Emotion_Specialized','Perplexity_Emotion_Specialized']]

import ast
import pandas as pd

def extract_emotion_details(emotion_analysis):
    """
    Extracts emotion label and score from the model output.
    Handles different input formats including strings and lists of dictionaries.
    """
    # Check if the input is a string, and safely parse it if so
    if isinstance(emotion_analysis, str):
        try:
            emotion_dict = ast.literal_eval(emotion_analysis)
        except (SyntaxError, ValueError):
            return None, None  # Return None if parsing fails
    else:
        # Use directly if it's already a dictionary or list
        emotion_dict = emotion_analysis

    # Handle cases where the result is a list of lists or list of dicts
    if isinstance(emotion_dict, list):
        # If it's a list of lists, get the first dict within the nested lists
        if isinstance(emotion_dict[0], list):
            emotion_dict = emotion_dict[0][0]
        else:
            # If it's a list of dictionaries, get the first dictionary
            emotion_dict = emotion_dict[0]

    # Extract label and score if they exist
    if isinstance(emotion_dict, dict) and 'label' in emotion_dict and 'score' in emotion_dict:
        return emotion_dict['label'], emotion_dict['score']
    else:
        return None, None  # Return None if expected keys are missing

# Apply the extraction function and expand the results into separate columns
emotion_columns = {
    'Human_Emotion_Specialized': ['Human_Emotion_Label', 'Human_Emotion_Score'],
    'ChatGPT_Emotion_Specialized': ['ChatGPT_Emotion_Label', 'ChatGPT_Emotion_Score'],
    'Gemini_Emotion_Specialized': ['Gemini_Emotion_Label', 'Gemini_Emotion_Score'],
    'Copilot_Emotion_Specialized': ['Copilot_Emotion_Label', 'Copilot_Emotion_Score'],
    'Claude_Emotion_Specialized': ['Claude_Emotion_Label', 'Claude_Emotion_Score'],
    'Perplexity_Emotion_Specialized': ['Perplexity_Emotion_Label', 'Perplexity_Emotion_Score'],
}

# Process each emotion column and add extracted label and score to the DataFrame
for col, new_cols in emotion_columns.items():
    df[new_cols] = df[col].apply(lambda x: pd.Series(extract_emotion_details(x)))

# Check the updated DataFrame
df.head()

# Define the columns to be saved
columns_to_save = [
    'Serial No.', 'Question', 'Human Answer', 'OpenAI Chatgpt Answer',
    'Google Gemini Answer', 'Microsoft Copilot Answer', 'Claude Answer',
    'Perplexity Answer', 'Human_Emotion_Specialized', 'ChatGPT_Emotion_Specialized',
    'Gemini_Emotion_Specialized', 'Copilot_Emotion_Specialized', 'Claude_Emotion_Specialized',
    'Perplexity_Emotion_Specialized', 'Human_Emotion_Label', 'Human_Emotion_Score',
    'ChatGPT_Emotion_Label', 'ChatGPT_Emotion_Score', 'Gemini_Emotion_Label',
    'Gemini_Emotion_Score', 'Copilot_Emotion_Label', 'Copilot_Emotion_Score',
    'Claude_Emotion_Label', 'Claude_Emotion_Score', 'Perplexity_Emotion_Label',
    'Perplexity_Emotion_Score'
]

# Save only the specified columns to a CSV file
df[columns_to_save].to_csv('emotion_analysis_results.csv', index=False)

df.columns

import matplotlib.pyplot as plt
import seaborn as sns

# Create a DataFrame to hold the emotion counts for each model
emotion_labels = ['Human_Emotion_Label', 'ChatGPT_Emotion_Label', 'Gemini_Emotion_Label', 'Copilot_Emotion_Label', 'Claude_Emotion_Label', 'Perplexity_Emotion_Label']

# Reshape the data for plotting
emotion_counts = pd.DataFrame()
for col in emotion_labels:
    emotion_counts[col] = df[col]

# Unstack and count the frequency of each emotion per model
emotion_distribution = emotion_counts.apply(pd.Series.value_counts).fillna(0).T

# Plot the emotion distribution
plt.figure(figsize=(10,6))
sns.heatmap(emotion_distribution, annot=True, cmap="Blues", fmt='.0f', cbar=False)
plt.title('Emotion Distribution Across Models')
plt.ylabel('Models')
plt.xlabel('Emotions')
plt.show()

import matplotlib.pyplot as plt

# Calculate the average confidence score for each source
avg_scores = {
    'Human': df['Human_Emotion_Score'].mean(),
    'ChatGPT': df['ChatGPT_Emotion_Score'].mean(),
    'Gemini': df['Gemini_Emotion_Score'].mean(),
    'Copilot': df['Copilot_Emotion_Score'].mean(),
    'Claude': df['Claude_Emotion_Score'].mean(),
    'Perplexity': df['Perplexity_Emotion_Score'].mean(),
}

# Plotting the average confidence scores
plt.figure(figsize=(10, 6))
plt.bar(avg_scores.keys(), avg_scores.values(), color=['black', 'teal', 'royalblue', 'skyblue', 'salmon', 'darkcyan'], edgecolor='black')
plt.title('Average Confidence Score by Response Source', fontsize=16)
plt.xlabel('Response Source', fontsize=12)
plt.ylabel('Average Confidence Score', fontsize=12)
plt.ylim(0, 1)  # Scores range from 0 to 1
plt.xticks(rotation=0)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import pandas as pd

# Calculate a moving average for each model (e.g., 10-sample window)
smoothed_scores = emotion_scores.rolling(window=10, center=True).mean()

# Plot smoothed emotion scores
plt.figure(figsize=(12, 6))

for model in smoothed_scores.columns:
    if model == 'Human_Emotion_Score':
        plt.plot(smoothed_scores.index, smoothed_scores[model], label=model, color='black', linewidth=2)
    else:
        plt.plot(smoothed_scores.index, smoothed_scores[model], label=model)

# Add labels and title
plt.xlabel('Sample Index')
plt.ylabel('Emotion Score')
plt.title('Smoothed Emotion Scores Across Models (10-Sample Moving Average)')
plt.legend()
plt.show()

import seaborn as sns

# Plot a box plot for each model's emotion score distribution
plt.figure(figsize=(10, 6))
sns.boxplot(data=emotion_scores, palette="Set2")

# Add labels and title
plt.xlabel('Model')
plt.ylabel('Emotion Score')
plt.title('Distribution of Emotion Scores Across Models')
plt.show()